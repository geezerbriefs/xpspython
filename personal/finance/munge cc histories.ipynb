{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "import datetime\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chase statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_20 (string):\n",
    "    \"\"\"\n",
    "    makes 2 digit 2000's years ('17 or '10) into the 4 digit equivalent. If it's already 4 digit, leave it alone.\n",
    "    \n",
    "    Takes (string) which is the full date string of format mm/dd/yy or mm/dd/yyyy\n",
    "    \"\"\"\n",
    "    #check if it's a 2 digit year\n",
    "    if string[-3] == '/':\n",
    "        \n",
    "        return string[:-2] + '20' + string[-2:]\n",
    "    \n",
    "    #if it's already a 4 digit year, return as is\n",
    "    elif string[-3] == '0':\n",
    "        \n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtime (string):\n",
    "    \"\"\"\n",
    "    converts strings to datetime objects using datetime.strptime mostly just wrote this so I could .apply() it in pandas\n",
    "    method chains\n",
    "    \n",
    "    takes (string) which is the full date of format mm/dd/yyyy (can't be 2 digit year)\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(string, '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_close (df_from_pdf):\n",
    "    \"\"\"\n",
    "    produces a list of series slices [opening datetime, closing datetime] by searching for the (hopefully)\n",
    "    single row where \"opening/closing date\" is listed, hopefully with the actual values as a separate cell\n",
    "    than \"opening/closing date\" string\n",
    "    ---Takes---\n",
    "    df_from_pdf : a whole pdf derived credit card statement as a dataframe\n",
    "    \n",
    "    ---Returns---\n",
    "    dates  :  [opening datetime, closing datetime] as series slices (I hope this is ok)\n",
    "    \"\"\"\n",
    "    \n",
    "    #copy so not mod orig\n",
    "    df = df_from_pdf.copy()\n",
    "    \n",
    "    #get just the actual data in the row with opening and closing date\n",
    "    df = df.dropna(axis=0, how='all').loc[df['Unnamed: 0'] == 'Opening/Closing Date', :].dropna(axis=1, how='all')\n",
    "    \n",
    "    #columns are default column titles that are annoying, rename to numbers\n",
    "    df.columns = [i for i in range(len(df.columns))]\n",
    "    \n",
    "    #df now has 2 columns [0] = 'opening/closing date', [1] = 'mm/dd/yy - mm/dd/yy'\n",
    "    open_close = df[1].str.split('-', expand=True)\n",
    "    \n",
    "    dates = []\n",
    "    for i in range(len(open_close.columns)):\n",
    "        dates.append(open_close[i].str.strip().apply(add_20).apply(get_dtime).values[0])\n",
    "    \n",
    "    #dates has open date first and close date second\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date_string (entry):\n",
    "    \n",
    "    \"\"\"\n",
    "    decides whether or not a string matches the pattern 'mm/dd...'\n",
    "    just checks if that patterns is present at the beginning of the string, doesnt check for end of string\n",
    "    because statements sometimes have concatenated a bunch of strings into one giant entry that just begins\n",
    "    with mm/dd\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(entry, str):\n",
    "    \n",
    "        bb = re.match(r\"([0-9]{1,2}?)/([0-9]{1,2}?)\", entry)\n",
    "\n",
    "        if bb:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_desc_price (row_of_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes a row of a dataframe containing one transaction's information\n",
    "    the row must have at least 3 separate columns (others holding NaNs) with date, description, price\n",
    "    they must be in that order\n",
    "    \n",
    "    returns a dictionary with {'date':date, 'desc':description, 'price':price} for use in making dataframes later\n",
    "    \"\"\"\n",
    "    \n",
    "    #have to drop on index because a row slice is also a series object which doesn't have multiple columns\n",
    "    row = row_of_df.dropna(axis='index', how='all')\n",
    "    \n",
    "    #create tuples that pair date/desc/price to the appropriate item extracted from the row\n",
    "    #and make a dictionary where date/desc/price : appropriate data. This makes it easier to\n",
    "    #make dataframes later\n",
    "    date_desc_price_dict = {i:j for (i,j) in zip(['date', 'desc', 'price'], [item for item in row])}\n",
    "    \n",
    "\n",
    "    return date_desc_price_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_all_one_cell (df_row):\n",
    "    \"\"\"\n",
    "    if an entry has all the info smashed into the first ('date') cell in the excel sheet,\n",
    "    this function pulls out the date, desc and price and returns a dictionary that can be added\n",
    "    into the cleaned transaction dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    entry = df_row['date']\n",
    "    \n",
    "    sp = ' '\n",
    "    spaces = [pos for (pos, char) in enumerate(entry) if char == sp]\n",
    "    \n",
    "    first_space = spaces[0]\n",
    "    last_space = spaces[-1]\n",
    "    \n",
    "    date = entry[:first_space]\n",
    "    \n",
    "    try:\n",
    "        price = float(entry[last_space:])\n",
    "    except:\n",
    "        price = entry[last_space:]\n",
    "    \n",
    "    desc = entry[first_space:last_space].strip()\n",
    "    \n",
    "    \n",
    "    return {i:j for (i,j) in zip(['date', 'desc', 'price'], [date, desc, price])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_2_in_one (df_row):\n",
    "    \n",
    "    entry = df_row['date']\n",
    "    \n",
    "    sp = ' '\n",
    "    spaces = [pos for (pos, char) in enumerate(entry) if char == sp]\n",
    "    \n",
    "    first_space = spaces[0]\n",
    "    \n",
    "    date = df_row['date'][:first_space]\n",
    "    desc = df_row['date'][first_space:].strip()\n",
    "    price = df_row['desc']\n",
    "    \n",
    "    return {i:j for (i,j) in zip(['date', 'desc', 'price'], [date, desc, price])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_n_line (df):\n",
    "    \"\"\"\n",
    "    sometimes the first entry in the date column is concatenated with a newline ('\\n') and Purchases\n",
    "    This splits at the new line and takes just the date\n",
    "    \"\"\"\n",
    "    for row in df.index:\n",
    "    \n",
    "        entry = df.loc[row, 'date'] \n",
    "\n",
    "        if '\\n' in entry:\n",
    "            df.loc[row, 'date'] = entry.split('\\n')[0]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_date_desc_price (raw_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes in a raw statement df read in from the xlsx file, searches using regex for dates of form 'mm/dd' in the\n",
    "    first column, then extracts just those rows, cleans them up, uses function date_desc_price() to attempt to\n",
    "    extract and label each piece of information appropriately and returns a dataframe with just the transaction\n",
    "    information. Because of variable formatting in the parent xlsx, there will be errors, which I clean up later.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get the indices in the statement where the first columns entry is a date, which should identify only transaction rows\n",
    "    indx_where_trans = raw_df['Unnamed: 0'].apply(is_date_string)\n",
    "\n",
    "    #get these transactions as a slice with a fresh index and no junky NaNs\n",
    "    just_trans = (raw_df.loc[indx_where_trans, :]\n",
    "                  .dropna(axis='columns', how='all')\n",
    "                  .reset_index(drop=True)\n",
    "                 )\n",
    "\n",
    "    #make a list of dictionaries that hold all the date/desc/price info\n",
    "    trans_dicts=[]\n",
    "    for row in just_trans.index:\n",
    "\n",
    "        trans_dicts.append(date_desc_price(just_trans.loc[row]))\n",
    "        \n",
    "    #make a nice clean dataframe with the information you want\n",
    "    poss_err_trans = remove_n_line(pd.DataFrame(trans_dicts))\n",
    "    \n",
    "    return poss_err_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_squishing_errors (date_desc_price_df):\n",
    "    \"\"\"\n",
    "    there is variability in the parent xlsx documents that concatenates description and sometimes price to the date\n",
    "    column, this checks for these errors by looking for NaN's in the transactions dataframe and then doing the proper\n",
    "    correction based on the NaN pattern. The result is an dataframe with all the same transactions, but with concat\n",
    "    errors fixed. The index is out of order as a result of this, can easily sort to fix.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index in date_desc_price_df.index:\n",
    "        \n",
    "        #get a single row, may have NaNs in fields that have been concatenated into fields to the left\n",
    "        h = date_desc_price_df.loc[index]\n",
    "\n",
    "        #NaN's aren't equal to each other so this test will check for them\n",
    "        #without triggering the weird np.isnan error when checking strings\n",
    "        test = [item==item for item in h]\n",
    "        \n",
    "        #complete row\n",
    "        if test == [True, True, True]:\n",
    "            pass\n",
    "        #date and desc squashed together, price is in desc\n",
    "        elif test == [True, True, False]:\n",
    "            date_desc_price_df = pd.concat([date_desc_price_df, pd.DataFrame(expand_2_in_one(h), index=[index])], axis='index')\n",
    "        #all are squashed in first column\n",
    "        elif test == [True, False, False] or test == [True]:\n",
    "            date_desc_price_df = pd.concat([date_desc_price_df, pd.DataFrame(expand_all_one_cell(h), index=[index])], axis='index')\n",
    "            \n",
    "    date_desc_price_df = date_desc_price_df.dropna(axis='index', how='any')\n",
    "    \n",
    "    return date_desc_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_YEN (trans_df):\n",
    "    \"\"\"\n",
    "    due to japan trip, there are some lines that have a date and the word \"YEN\" in them, but arent transactions\n",
    "    remove these\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in trans_df.index:\n",
    "        #get a row\n",
    "        r = trans_df.loc[row]\n",
    "        \n",
    "        for x in r:\n",
    "            if 'YEN' in str(x):\n",
    "                trans_df = trans_df.drop(row, axis=0)\n",
    "    \n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_trans_date (trans_df):\n",
    "    \"\"\"\n",
    "    transaction dates dont have years right now, sometimes transactions are from multiple years \n",
    "    (some are in 12/dd/2016, some are in 01/dd/2017). Add the proper year to the date column and\n",
    "    extract that date to datetime\n",
    "    \"\"\"\n",
    "    \n",
    "    #check if this statement spans the change of a year\n",
    "    c = trans_df.loc[0, 'st close'].year\n",
    "    o = trans_df.loc[0, 'st open'].year\n",
    "    \n",
    "    year_diff = c - o\n",
    "    \n",
    "    #the statement stays within the same year\n",
    "    if year_diff == 0:\n",
    "        #doesn't matter which year to append\n",
    "        trans_df['date'] = trans_df['date'] + '/' + str(c)\n",
    "        \n",
    "    #we change over years in this statement\n",
    "    elif year_diff == 1:\n",
    "        #where are the january transactions that should get the updated date\n",
    "        where = ['01/' in d for d in trans_df['date']]\n",
    "        \n",
    "        #replace the january transactions with the closing date's year, which should be advanced\n",
    "        trans_df.loc[where, 'date'] = trans_df.loc[where, 'date'] + '/' + str(c)\n",
    "        \n",
    "        #the one's that shouldn't get the advanced year\n",
    "        not_where = [not x for x in where]\n",
    "        \n",
    "        #add the not advanced year to these dates\n",
    "        trans_df.loc[not_where, 'date'] = trans_df.loc[not_where, 'date'] + '/' + str(o)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('this statement appears to span multiple years, that doesnt make sense')\n",
    "        \n",
    "    trans_df['date'] = trans_df['date'].apply(get_dtime)\n",
    "    \n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_one_statement (filepath):\n",
    "    #open data\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    #get the opening and closing dates as a list [open, close]\n",
    "    o_c = open_close(df)\n",
    "    \n",
    "    #get the cleaned up, but still error prone transctions df\n",
    "    poss_err_trans = extract_all_date_desc_price(df)\n",
    "    \n",
    "    #correct the concatenation errors\n",
    "    trans_clean = correct_squishing_errors(poss_err_trans)\n",
    "    \n",
    "    #add the statement association information\n",
    "    trans_clean['st open'] = o_c[0]\n",
    "    trans_clean['st close'] = o_c[1]\n",
    "    \n",
    "    #remove any lines that have \"YEN\" because they aren't transactions\n",
    "    trans_clean = remove_YEN(trans_clean)\n",
    "    \n",
    "    #complete and make datetime of the transaction date column\n",
    "    trans_clean = complete_trans_date(trans_clean)\n",
    "    \n",
    "    return trans_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_sts_together (directory_with_st_xlsx):\n",
    "    \n",
    "    sts = glob.glob(directory_with_st_xlsx + '/*x8723-.xlsx')\n",
    "    \n",
    "    all_sts = []\n",
    "    for statement_xlsx in sts:\n",
    "        \n",
    "        all_sts.append(munge_one_statement(statement_xlsx))\n",
    "        \n",
    "    full_hist = pd.concat(all_sts, axis=0)\n",
    "    \n",
    "    return full_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../../Finances/cc info/chase(-8723) statements/csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geeze\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = all_sts_together(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#last statement was all buggy, so i just manually got the info out\n",
    "last = complete_trans_date(pd.read_excel('../../../Finances/cc info/chase(-8723) statements/csv/clean st 20181008.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put together\n",
    "full_hist = pd.concat([hist, last], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hist = full_hist.sort_values(by='date').reset_index(drop=True)\n",
    "full_hist['card'] = 'chase -8723'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>price</th>\n",
       "      <th>st open</th>\n",
       "      <th>st close</th>\n",
       "      <th>card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-285.91</td>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-1401.21</td>\n",
       "      <td>2016-08-09</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-333.94</td>\n",
       "      <td>2016-09-09</td>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-999.56</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>2016-11-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-453.86</td>\n",
       "      <td>2016-11-09</td>\n",
       "      <td>2016-12-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-395.25</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-695.5</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-833.92</td>\n",
       "      <td>2017-02-09</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-967.29</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-1126.64</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2017-06-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-874.42</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>2017-06-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-146.77</td>\n",
       "      <td>2017-06-09</td>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-77.85</td>\n",
       "      <td>2017-07-09</td>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2017-09-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-130.52</td>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2017-10-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-168.48</td>\n",
       "      <td>2017-09-09</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-4.71</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-36.07</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-503.55</td>\n",
       "      <td>2018-03-09</td>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-34.72</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2018-08-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-13.32</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>2018-08-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-26.78</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>AUTOMATIC PAYMENT - THANK YOU</td>\n",
       "      <td>-43.9</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>chase -8723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                           desc    price    st open   st close  \\\n",
       "28  2016-08-05  AUTOMATIC PAYMENT - THANK YOU  -285.91 2016-07-09 2016-08-08   \n",
       "46  2016-09-05  AUTOMATIC PAYMENT - THANK YOU -1401.21 2016-08-09 2016-09-08   \n",
       "70  2016-10-05  AUTOMATIC PAYMENT - THANK YOU  -333.94 2016-09-09 2016-10-08   \n",
       "88  2016-11-04  AUTOMATIC PAYMENT - THANK YOU  -999.56 2016-10-09 2016-11-08   \n",
       "107 2016-12-05  AUTOMATIC PAYMENT - THANK YOU  -453.86 2016-11-09 2016-12-08   \n",
       "141 2017-01-05  AUTOMATIC PAYMENT - THANK YOU  -395.25 2016-12-09 2017-01-08   \n",
       "164 2017-02-05  AUTOMATIC PAYMENT - THANK YOU   -695.5 2017-01-09 2017-02-08   \n",
       "183 2017-03-05  AUTOMATIC PAYMENT - THANK YOU  -833.92 2017-02-09 2017-03-08   \n",
       "210 2017-04-05  AUTOMATIC PAYMENT - THANK YOU  -967.29 2017-03-09 2017-04-08   \n",
       "224 2017-05-05  AUTOMATIC PAYMENT - THANK YOU -1126.64 2017-04-09 2017-05-08   \n",
       "231 2017-06-05  AUTOMATIC PAYMENT - THANK YOU  -874.42 2017-05-09 2017-06-08   \n",
       "235 2017-07-05  AUTOMATIC PAYMENT - THANK YOU  -146.77 2017-06-09 2017-07-08   \n",
       "242 2017-08-04  AUTOMATIC PAYMENT - THANK YOU   -77.85 2017-07-09 2017-08-08   \n",
       "254 2017-09-05  AUTOMATIC PAYMENT - THANK YOU  -130.52 2017-08-09 2017-09-08   \n",
       "261 2017-10-05  AUTOMATIC PAYMENT - THANK YOU  -168.48 2017-09-09 2017-10-08   \n",
       "265 2018-02-05  AUTOMATIC PAYMENT - THANK YOU    -4.71 2018-01-09 2018-02-08   \n",
       "277 2018-03-05  AUTOMATIC PAYMENT - THANK YOU   -36.07 2018-02-09 2018-03-08   \n",
       "280 2018-04-05  AUTOMATIC PAYMENT - THANK YOU  -503.55 2018-03-09 2018-04-08   \n",
       "281 2018-05-04  AUTOMATIC PAYMENT - THANK YOU   -34.72 2018-04-09 2018-05-08   \n",
       "292 2018-08-05  AUTOMATIC PAYMENT - THANK YOU   -13.32 2018-07-09 2018-08-08   \n",
       "296 2018-09-05  AUTOMATIC PAYMENT - THANK YOU   -26.78 2018-08-09 2018-09-08   \n",
       "302 2018-10-05  AUTOMATIC PAYMENT - THANK YOU    -43.9 2018-09-09 2018-10-08   \n",
       "\n",
       "            card  \n",
       "28   chase -8723  \n",
       "46   chase -8723  \n",
       "70   chase -8723  \n",
       "88   chase -8723  \n",
       "107  chase -8723  \n",
       "141  chase -8723  \n",
       "164  chase -8723  \n",
       "183  chase -8723  \n",
       "210  chase -8723  \n",
       "224  chase -8723  \n",
       "231  chase -8723  \n",
       "235  chase -8723  \n",
       "242  chase -8723  \n",
       "254  chase -8723  \n",
       "261  chase -8723  \n",
       "265  chase -8723  \n",
       "277  chase -8723  \n",
       "280  chase -8723  \n",
       "281  chase -8723  \n",
       "292  chase -8723  \n",
       "296  chase -8723  \n",
       "302  chase -8723  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_hist.loc[full_hist['desc'] == 'AUTOMATIC PAYMENT - THANK YOU', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hist.to_csv('../../../Finances/cc info/chase(-8723) statements/chase_up_to_20181008.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
