{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting things\n",
    "\n",
    "#%matplotlib qt5 -- I don't know what this is\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "#All of Anandh's customized seaborn/matplotlib settings\n",
    "\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_style({\"xtick.direction\": \"in\",\"ytick.direction\": \"in\"})\n",
    "\n",
    "#%config InlineBackend.figure_f.ormats=['svg']\n",
    "\n",
    "mpl.rc('axes', prop_cycle=(cycler('color', ['r', 'k', 'b','g','y','m','c']) ))\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "#mpl.rc('text', usetex=False)\n",
    "#mpl.rc('text.latex', preamble=r'\\usepackage{helvet}\n",
    "#\\renewcommand\\familydefault{\\sfdefault}\\usepackage{sansmath}\\sansmath')\n",
    "\n",
    "    #If you want to use a different font\n",
    "# mpl.rc('font',**{'family':'sans-serif','sans-serif':['Helvetica'], \n",
    "#                  'serif': ['Helvetica']})\n",
    "\n",
    "tw = 1.5\n",
    "sns.set_style({\"xtick.major.size\": 3, \"ytick.major.size\": 3,\n",
    "               \"xtick.minor.size\": 2, \"ytick.minor.size\": 2,\n",
    "               'axes.labelsize': 16, 'axes.titlesize': 16,\n",
    "               'xtick.major.width': tw, 'xtick.minor.width': tw,\n",
    "               'ytick.major.width': tw, 'ytick.minor.width': tw})\n",
    "\n",
    "mpl.rc('xtick', labelsize=14) \n",
    "mpl.rc('ytick', labelsize=14)\n",
    "mpl.rc('axes', linewidth=1.5)\n",
    "mpl.rc('legend', fontsize=14)\n",
    "mpl.rc('figure', figsize=(9,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Z:/Reed/Projects/micro_consortia/DARPA_biocon/Task 1.1/A=B/20190214 A=B mar cfp yfp small screen 1/'\n",
    "\n",
    "filename = '20190214 A=B mar 1.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['OD700_raw', 'CFP_raw', 'YFP_raw', 'OD700', 'CFP', 'YFP', 'OD700_tidy', 'CFP_tidy', 'YFP_tidy', 'dense_norm_cumsum_tidy', 'IDs', 'Exp'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.read_excel(directory + filename, sheet_name=None)\n",
    "\n",
    "dd.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create condensed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_fluor_and_od700 (dict_of_data):\n",
    "    \n",
    "    #get the data sheets you want to join\n",
    "    useful_sheets = [x for x in dd.keys() if '_tidy' in x]\n",
    "\n",
    "    #find the od sheet, which will be the master one\n",
    "    od_sheet = [x for x in useful_sheets if 'OD700' in x]\n",
    "\n",
    "    #get the others using od_sheet\n",
    "    others = [x for x in useful_sheets if x not in od_sheet]\n",
    "    \n",
    "    #now that od_sheet is used, pop the value out of the list\n",
    "    if len(od_sheet) == 1:\n",
    "        od_sheet = od_sheet[0]\n",
    "    else:\n",
    "        raise ValueError(\"looking for the od_sheet with 'OD700' found more than one data sheet\")\n",
    "\n",
    "    \n",
    "    #get the od time column\n",
    "    ot = np.sort(np.unique(dict_of_data[od_sheet]['Time']))\n",
    "    \n",
    "    #rename od stuff\n",
    "    dict_of_data[od_sheet] = dict_of_data[od_sheet].rename({'value' : od_sheet.lower().replace('_tidy', '')}, axis='columns')\n",
    "    \n",
    "    #for the remaining sheets of data you want to join\n",
    "    for sheet in others:\n",
    "        #get this one's time values\n",
    "        ft = np.sort(np.unique(dict_of_data[sheet]['Time']))\n",
    "        \n",
    "        #create dict to identify fluor time column entries with od time columns entries\n",
    "        #every time you see time (fluor time entry), replace it with (od time entry)\n",
    "        time_replacement_dict = {x : y for x,y in zip(ft,ot)}\n",
    "    \n",
    "        #replace the time identifier column in the fluor df\n",
    "        dict_of_data[sheet]['Time'] = dict_of_data[sheet]['Time'].map(time_replacement_dict)\n",
    "        \n",
    "        #do some renaming\n",
    "        dict_of_data[sheet] = dict_of_data[sheet].rename({'value' : sheet.lower().replace('_tidy', '')}, axis='columns')\n",
    "    \n",
    "\n",
    "    #load all data for joining\n",
    "    dataframes = [dict_of_data[sheet] for sheet in useful_sheets]\n",
    "    \n",
    "    all_data_joined = pd.concat(dataframes, axis='columns')\n",
    "    \n",
    "    all_data_joined = all_data_joined.loc[:,~all_data_joined.columns.duplicated()]\n",
    "    \n",
    "    #fix overflow errors\n",
    "    all_data_joined = all_data_joined.replace(\"OVRFLW\", 99999)\n",
    "    \n",
    "    return all_data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = join_fluor_and_od700(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create od normalized fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create od normalized fluor data\n",
    "\n",
    "dense_n = dense.copy()\n",
    "\n",
    "dense_n['cfp_norm'] = dense_n['cfp'] / dense_n['od700']\n",
    "dense_n['yfp_norm'] = dense_n['yfp'] / dense_n['od700']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cumulative fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_fluor (df):\n",
    "    \"\"\"\n",
    "    create cumulative summed fluor data for fluorescence and od normalized fluorescence, append it to big datasheet\n",
    "    \"\"\"\n",
    "    \n",
    "    #get the columns you want to sum\n",
    "    sum_these = [x for x in df.columns if 'fp' in x] #works only if target columns are unique in containing 'fp'\n",
    "    \n",
    "    #start with the orig df in the list so you can just give the final list to pd.concat\n",
    "    add_these = []\n",
    "    for well in np.unique(df['well']):\n",
    "\n",
    "        i = df['well'] == well\n",
    "\n",
    "        #sort values by time so you get the cum sum of the values in the correct time order\n",
    "        sums = df.loc[i].sort_values('Time').loc[:, sum_these].cumsum()\n",
    "\n",
    "        sums.columns = sums.columns + '_sum'\n",
    "        \n",
    "        add_these.append(sums)\n",
    "    \n",
    "    sums_together = pd.concat(add_these, axis='index')\n",
    "    \n",
    "    df_plus_sums = pd.concat([df, sums_together], axis='columns')\n",
    "    \n",
    "    return df_plus_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cfp', 'yfp', 'cfp_norm', 'yfp_norm']\n"
     ]
    }
   ],
   "source": [
    "dense_n_sum = cumsum_fluor(dense_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_n_sum.to_csv(directory + 'condensed_normed_cumsum_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a 2 stage non cum CFP YFP trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
